{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "\n",
    "from sortify import *\n",
    "from graph import *\n",
    "\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(figsize=(15,10))\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# files = glob.glob('cache/*')\n",
    "# for f in files:\n",
    "#     os.remove(f)\n",
    "\n",
    "try:\n",
    "    tracks = pd.read_pickle('cache/tracks.pkl')\n",
    "except FileNotFoundError:\n",
    "    print('Fetching user tracks...')\n",
    "    tracks = get_user_tracks()\n",
    "    tracks.to_pickle('cache/tracks.pkl')\n",
    "    \n",
    "try:\n",
    "    artists = pd.read_pickle('cache/artists.pkl')\n",
    "except FileNotFoundError:\n",
    "    print('Fetching artist data...')\n",
    "    artists = get_artist_data(tracks.artist.unique())\n",
    "    artists.to_pickle('cache/artists.pkl')\n",
    "\n",
    "try:\n",
    "    G = nx.read_graphml('related_artists.graphml')\n",
    "except FileNotFoundError:\n",
    "    G = initialize_graph(artists.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2657/2657 [00:07<00:00, 358.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All artists processed (0 fetched, 2548 skipped, 109 missing)\n",
      "0 artists with incomplete data found\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "not_found = 0\n",
    "skipped = 0\n",
    "missing = 0\n",
    "missing_artists = []\n",
    "\n",
    "for artist_id in tqdm(all_artists):\n",
    "    if artist_fetched(G, artist_id):\n",
    "        skipped += 1\n",
    "    else:\n",
    "        related_list = sp.artist_related_artists(artist_id)['artists']\n",
    "        if len(related_list) <= 1:\n",
    "            missing += 1\n",
    "            missing_artists.append(artist_id)\n",
    "            continue\n",
    "        source = add_vertex(G, artist_id)\n",
    "        source['fetched'] = True\n",
    "        source['alias'] = artist_objects[artist_id]['name']\n",
    "        source['popularity'] = artist_popularity[artist_id]\n",
    "        not_found += 1\n",
    "\n",
    "        for related in related_list:\n",
    "            artist_objects[related['id']] = related\n",
    "            target = add_edge(G, artist_id, related['id'])[1]\n",
    "            target['alias'] = related['name']\n",
    "            target['popularity'] = related['popularity']\n",
    "            if target['fetched'] is None:\n",
    "                target['fetched'] = False\n",
    "\n",
    "    i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         print(\"%d/%d artists processed, %d fetched, %d skipped, %d missing\" % (i, len(all_artists), not_found, skipped, missing))\n",
    "print(\"All artists processed (%d fetched, %d skipped, %d missing)\" % (not_found, skipped, missing))\n",
    "        \n",
    "for artist in missing_artists:\n",
    "    del artist_tracks[artist]\n",
    "    all_artists.remove(artist)\n",
    "        \n",
    "for artist in list(artist_tracks.keys()):\n",
    "    try:\n",
    "        G.vs.find(artist)\n",
    "    except ValueError:\n",
    "        del artist_tracks[artist]\n",
    "        all_artists.remove(artist)\n",
    "\n",
    "incomplete_artists = []\n",
    "for artist_id in all_artists:\n",
    "    artist = G.vs.find(artist_id)\n",
    "    if not artist['alias'] or artist['popularity'] is None:\n",
    "        try:\n",
    "            artist['alias'] = artist_objects[artist['name']]['name']\n",
    "            artist['popularity'] = artist_objects[artist['name']]['popularity']\n",
    "        except KeyError:\n",
    "            incomplete_artists.append(artist_id)\n",
    "    elif np.isnan(artist['popularity']):\n",
    "        artist['popularity'] = 0\n",
    "\n",
    "print(\"%s artists with incomplete data found\" % len(incomplete_artists))\n",
    "if incomplete_artists:\n",
    "    print(\"fetching data...\")\n",
    "for chunk in chunks(50, incomplete_artists):\n",
    "    for artist in sp.artists(chunk)['artists']:\n",
    "        artist_vertex = G.vs.find(artist['id'])\n",
    "        artist_vertex['alias'] = artist['name']\n",
    "        artist_vertex['popularity'] = artist['popularity']\n",
    "\n",
    "ig.write(G, 'related_artists.graphml', format='graphml')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T22:28:31.181783Z",
     "start_time": "2021-04-21T22:28:31.177311Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artist_names = dict()\n",
    "for artist in artist_tracks:\n",
    "    v = G.vs.find(artist)\n",
    "    artist_names[artist] = v['alias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 disconnected components\n",
      "{<igraph.Graph object at 0x00000232B58F84F8>: 222495}\n"
     ]
    }
   ],
   "source": [
    "components = G.components().subgraphs()\n",
    "sizes = {}\n",
    "for g in components:\n",
    "    sizes[g] = len(g.vs)\n",
    "disconnected = sorted(components, key=lambda x: sizes[x])[:-1]\n",
    "print('%d disconnected components' % len(disconnected))\n",
    "print(sizes)\n",
    "\n",
    "# non_fetched = []\n",
    "# for g in disconnected:\n",
    "#     for v in g.vs:\n",
    "#         if not v['fetched']:\n",
    "#             non_fetched.append(v['name'])\n",
    "# #         G.vs.find(v['name'])['fetched'] = False\n",
    "# print(non_fetched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components, points = G.biconnected_components(return_articulation_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in points:\n",
    "    v = G.vs[p]\n",
    "    print('%s pop=%d, deg=%d' % (v['alias'], v['popularity'], v.degree()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erroneous = 0\n",
    "i = 0\n",
    "artists = G.vs.select(popularity=None)['name']\n",
    "for chunk in chunks(50, artists):\n",
    "    for artist in sp.artists(chunk)['artists']:\n",
    "        try:\n",
    "            G.vs.find(artist['id'])['popularity'] = artist['popularity']\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            erroneous += 1\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print('%d/%d artists processed, %d erroneous artists returned' % (i, len(artists), erroneous))\n",
    "            ig.write(G, 'related_artists.graphml', format='graphml')\n",
    "            \n",
    "ig.write(G, 'related_artists.graphml', format='graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "# df = pd.DataFrame({'deg': G.vs.degree(), 'pop': G.vs['popularity']})\n",
    "for deg in G.vs.degree():\n",
    "    if deg <= 5:\n",
    "        counts[deg] += 1\n",
    "        \n",
    "print(counts)\n",
    "\n",
    "# sns.distplot(G.vs.select(name_in=all_artists)['popularity'], rug=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand graph\n",
    "g_artists = G.vs['name']\n",
    "\n",
    "print('Current graph: %d nodes, %d edges' % (len(G.vs), len(G.es)))\n",
    "print('Processing %d artists in graph...' % len(g_artists))\n",
    "processed = 0\n",
    "fetched = 0\n",
    "skipped = 0\n",
    "\n",
    "for i in range(len(g_artists)):\n",
    "    if g_artists[i] in fetched_artists:\n",
    "        skipped += 1\n",
    "        \n",
    "    else:\n",
    "        related_list = sp.artist_related_artists(g_artists[i])['artists']\n",
    "        not_found += 1\n",
    "        fetched_artists.add(g_artists[i])\n",
    "\n",
    "        for related in related_list:\n",
    "            artist_names[related['id']] = related['name']\n",
    "            add_edge(G, g_artists[i], related['id'])\n",
    "            \n",
    "    if i % 1000 == 0:\n",
    "        print(\"%d/%d artists processed, %d fetched, %d skipped\" % (i, len(g_artists), not_found, skipped))\n",
    "        \n",
    "    if not_found % 1000 == 0:\n",
    "        print(\"Saving graph...\")\n",
    "        ig.write(G, 'related_artists.graphml', format='graphml')\n",
    "        with open('fetched_artists', 'wb') as file:\n",
    "            pickle.dump(fetched_artists, file)\n",
    "        with open('artist_names', 'wb') as file:\n",
    "            pickle.dump(artist_names, file)\n",
    "            \n",
    "print('New graph: %d nodes, %d edges' % (len(G.vs), len(G.es)))\n",
    "ig.write(G, 'related_artists.graphml', format='graphml')\n",
    "with open('fetched_artists', 'wb') as file:\n",
    "    pickle.dump(fetched_artists, file)\n",
    "with open('artist_names', 'wb') as file:\n",
    "    pickle.dump(artist_names, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(G.nodes)\n",
    "for i in range(len(degrees)):\n",
    "    degree = 0\n",
    "    artist = degrees[i]\n",
    "    for neighbor in nx.neighbors(G, artist):\n",
    "        if neighbor in artist_tracks:\n",
    "            degree += len(artist_tracks[neighbor])\n",
    "    degrees[i] = (artist, degree)\n",
    "\n",
    "degrees.sort(key=lambda tup: tup[1], reverse=True)\n",
    "degrees = [node for node in degrees if node[0] not in artist_tracks]\n",
    "degrees = degrees[:100]\n",
    "top_artists = [node[0] for node in degrees]\n",
    "\n",
    "for chunk in chunks(50, top_artists):\n",
    "    for artist in sp.artists(chunk)['artists']:\n",
    "        id = artist['id']\n",
    "        artist_genres[id] = artist['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph nodes: 222495\n",
      "Graph edges: 1164028\n",
      "Graph components: 1\n",
      "65.2 ms ± 1.62 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "0:00:05.274894\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "print(\"Graph nodes:\", len(G.vs))\n",
    "print(\"Graph edges:\", len(G.es))\n",
    "print(\"Graph components:\", len(G.components()))\n",
    "\n",
    "artists = list(artist_tracks.keys()) #+ top_artists\n",
    "artists_set = set(artists)\n",
    "distance = np.zeros((len(artists), len(artists)))\n",
    "\n",
    "start = datetime.now()\n",
    "# distance = G.shortest_paths(source=artists, target=artists)\n",
    "%timeit G.shortest_paths(source=artists[2000])\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import sklearn.metrics\n",
    "\n",
    "# i = 0\n",
    "# for lengths in paths:\n",
    "#     source = artists[i]\n",
    "#     for j in range(len(artists)):\n",
    "#         target = artists[j]\n",
    "#         if target in lengths:\n",
    "#             distance[i][j] = lengths[target]\n",
    "#         else:\n",
    "#             distance[i][j] = np.inf\n",
    "#     iters += 1\n",
    "#     i += 1\n",
    "distance = np.array(distance)\n",
    "\n",
    "# connectivity = nx.to_numpy_matrix(G, nodelist=artists)\n",
    "similarity = np.exp(-distance / distance[ np.isfinite(distance) ].std())\n",
    "# for n_clusters in range(2, 13):\n",
    "# dist = np.copy(distance)\n",
    "# max_value = dist[ np.isfinite(dist) ].max()\n",
    "# print(max_value)\n",
    "# for i in range(len(dist)):\n",
    "#     for j in range(len(dist)):\n",
    "#         if dist[i][j] == np.inf:\n",
    "#             dist[i][j] = max_value * 2\n",
    "\n",
    "#     clustering = sklearn.cluster.AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='average', compute_full_tree=True).fit_predict(dist)\n",
    "# clustering = sklearn.cluster.DBSCAN(eps=3, min_samples=1, metric='precomputed', n_jobs=-1).fit_predict(dist)\n",
    "#     print(np.unique(clustering))\n",
    "#     print(\"%d cluster silhouette:\" % len(np.unique(clustering)), sklearn.metrics.silhouette_score(dist, clustering, metric='precomputed'))\n",
    "# print(sklearn.metrics.silhouette_score(dist, clustering, metric='precomputed'))\n",
    "\n",
    "clustering = sklearn.cluster.SpectralClustering(n_clusters=6, affinity='precomputed').fit_predict(similarity)\n",
    "# clustering = sklearn.cluster.AgglomerativeClustering(n_clusters=8, affinity='precomputed', linkage='average', compute_full_tree=True).fit_predict(dist)\n",
    "print(np.unique(clustering))\n",
    "artist_cluster = {}\n",
    "cluster_artists = {}\n",
    "for i in range(len(artists)):\n",
    "    artist_cluster[artists[i]] = clustering[i]\n",
    "    if clustering[i] not in cluster_artists:\n",
    "        cluster_artists[clustering[i]] = [artists[i]]\n",
    "    else:\n",
    "        cluster_artists[clustering[i]].append(artists[i])\n",
    "for clus in cluster_artists:\n",
    "    print(clus, len(cluster_artists[clus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_graphml('related_artists.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "nx.is_distance_regular(G)\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0:00:03.601370\n",
      "fluid: 0:02:28.887838\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "def print_mod(future):\n",
    "    print('modularity:', datetime.datetime.now() - start)\n",
    "def print_lpa(future):\n",
    "    print('lpa:', datetime.datetime.now() - start)\n",
    "def print_label(future):\n",
    "    print('label:', datetime.datetime.now() - start)\n",
    "def print_fluid(future):\n",
    "    print('fluid:', datetime.datetime.now() - start)\n",
    "\n",
    "futures = {}\n",
    "with ProcessPoolExecutor() as executor:\n",
    "#     futures['modularity'] = executor.submit(nx.algorithms.community.modularity_max.greedy_modularity_communities, G)\n",
    "#     futures['lpa'] = executor.submit(nx.algorithms.community.label_propagation.asyn_lpa_communities, G)\n",
    "    futures['label'] = executor.submit(nx.algorithms.community.label_propagation.label_propagation_communities, G)\n",
    "    futures['fluid'] = executor.submit(nx.algorithms.community.asyn_fluid.asyn_fluidc, G, 8)\n",
    "    \n",
    "#     futures['modularity'].add_done_callback(print_mod)\n",
    "#     futures['lpa'].add_done_callback(print_lpa)\n",
    "    futures['label'].add_done_callback(print_label)\n",
    "    futures['fluid'].add_done_callback(print_fluid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "# communities = nx.algorithms.community.label_propagation.label_propagation_communities(G)\n",
    "communities = nx.algorithms.community.centrality.girvan_newman(G)\n",
    "print(datetime.datetime.now() - start)\n",
    "\n",
    "cluster_artists = {}\n",
    "artist_clusters = {}\n",
    "\n",
    "label = 1\n",
    "for community in tqdm(communities):\n",
    "    cluster_artists[label] = []\n",
    "    for node in community:\n",
    "        id = G.nodes[node]['name']\n",
    "        cluster_artists[label].append(id)\n",
    "        artist_clusters[id] = label\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset...\n",
      "Clustering...\n",
      "1 169\n",
      "0 555\n",
      "3 258\n",
      "5 67\n",
      "2 1219\n",
      "4 280\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cluster\n",
    "import sklearn.mixture\n",
    "import pandas as pd\n",
    "\n",
    "artists = list(artist_tracks.keys())\n",
    "genres = set()\n",
    "for genre_list in artist_genres.values():\n",
    "    for genre in genre_list:\n",
    "        genres.add(genre)\n",
    "genres = list(genres)\n",
    "\n",
    "print('Building dataset...')\n",
    "X = np.zeros((len(artists), len(genres)))\n",
    "for i in range(len(artists)):\n",
    "    artist = artists[i]\n",
    "    for genre in artist_genres[artist]:\n",
    "        j = genres.index(genre)\n",
    "        X[i][j] = 1\n",
    "\n",
    "print('Clustering...')\n",
    "clustering = sklearn.mixture.GaussianMixture(n_components=6, covariance_type='full', tol=0.00001).fit_predict(X)\n",
    "\n",
    "artist_cluster = {}\n",
    "cluster_artists = {}\n",
    "for i in range(len(artists)):\n",
    "    artist_cluster[artists[i]] = clustering[i]\n",
    "    if clustering[i] not in cluster_artists:\n",
    "        cluster_artists[clustering[i]] = [artists[i]]\n",
    "    else:\n",
    "        cluster_artists[clustering[i]].append(artists[i])\n",
    "for clus in cluster_artists:\n",
    "    print(clus, len(cluster_artists[clus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Rave 67 tracks ['Boys Noize', 'Jamie xx', 'SBTRKT', 'Twin Shadow', 'Hot Chip', 'DJ Koze', 'Moderat', 'WhoMadeWho', 'Mr. Oizo', 'Mount Kimbie']\n",
      "EDM 1726 tracks ['deadmau5', 'Andrew Bayer', 'Porter Robinson', 'Chocolate Puma', 'DEAD BATTERY', 'Joran Van Pol', 'Killer Bee', 'ATTLAS', 'Lulu Rouge', 'Petit Biscuit']\n",
      "Electronic Trap 1288 tracks ['What So Not', 'Skrillex', 'UZ', 'Zeds Dead', 'Excision', 'NGHTMRE', 'San Holo', 'RL Grime', 'DJ Snake', 'Alison Wonderland']\n",
      "Pop 519 tracks ['Flume', 'Diplo', 'Isaiah Rashad', 'G-Eazy', 'The Chainsmokers', 'Stwo', 'Stephen', 'BURNS', 'Major Lazer', 'Justin Bieber']\n",
      "Tech House 1774 tracks ['Matt Lange', 'Noisia', 'Iglooghost', 'Lewis Fautzi', 'Pleasurekraft', 'Jon Hopkins', 'Maceo Plex', 'Gesaffelstein', 'Eprom', 'Dusky']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counts = {'cluster': [], 'genre': [], 'count': []}\n",
    "for cluster in cluster_artists:\n",
    "    genre_count = dict()\n",
    "    total_tracks = 0\n",
    "    for artist in cluster_artists[cluster]:\n",
    "        if artist not in artist_tracks:\n",
    "            continue\n",
    "        for genre in artist_genres[artist]:\n",
    "            if genre in genre_count:\n",
    "                genre_count[genre] += len(artist_tracks[artist])\n",
    "            else:\n",
    "                genre_count[genre] = len(artist_tracks[artist])\n",
    "        total_tracks += len(artist_tracks[artist])\n",
    "    for genre, count in genre_count.items():\n",
    "        counts['cluster'].append(cluster)\n",
    "        counts['genre'].append(genre)\n",
    "        counts['count'].append(count / total_tracks)\n",
    "\n",
    "counts = pd.DataFrame(counts)\n",
    "counts.sort_values('count', ascending=False, inplace=True)\n",
    "original_counts = counts.copy()\n",
    "\n",
    "clusters_to_name = set(counts['cluster'].unique())\n",
    "playlists = {}\n",
    "cluster_names = {}\n",
    "while clusters_to_name:\n",
    "    cluster = counts.iloc[0]['cluster']\n",
    "    name = counts.iloc[0]['genre']\n",
    "    if name == 'edm':\n",
    "        name = 'EDM'\n",
    "    else:\n",
    "        name = name.title()\n",
    "    playlists[name] = cluster\n",
    "    cluster_names[cluster] = name\n",
    "    counts = counts[ (counts['cluster']!=cluster) & (counts['genre']!=name) ]\n",
    "    clusters_to_name.remove(cluster)\n",
    "    \n",
    "artist_names = dict()\n",
    "for artist in artist_tracks:\n",
    "    v = G.vs.find(artist)\n",
    "    artist_names[artist] = v['alias']\n",
    "    \n",
    "playlist_tracks = dict()\n",
    "cluster_counts = dict()\n",
    "from collections import OrderedDict\n",
    "for name in playlists:\n",
    "    cluster_counts[name] = dict()\n",
    "    playlist_tracks[name] = []\n",
    "    cluster = playlists[name]\n",
    "    for artist in cluster_artists[cluster]:\n",
    "        if artist in artist_tracks:\n",
    "            cluster_counts[name][artist_names[artist]] = len(artist_tracks[artist])\n",
    "            playlist_tracks[name] += artist_tracks[artist]\n",
    "  \n",
    "for playlist, counts in cluster_counts.items():\n",
    "    artists_sorted = sorted(counts.keys(), key=lambda x: counts[x], reverse=True)\n",
    "    print(playlist, f'{len(playlist_tracks[playlist])} tracks', artists_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "            User authentication requires interaction with your\n",
      "            web browser. Once you enter your credentials and\n",
      "            give authorization, you will be redirected to\n",
      "            a url.  Paste that url you were directed to to\n",
      "            complete the authorization.\n",
      "\n",
      "        \n",
      "Opened https://accounts.spotify.com/authorize?client_id=b69a9985fa8842deb0691b2d0e3f0b69&response_type=code&redirect_uri=http%3A%2F%2Flocalhost%2F&scope=playlist-modify-private in your browser\n",
      "\n",
      "\n",
      "Enter the URL you were redirected to: http://localhost/?code=AQBRSjQCCdHhn28GaAUeAn4QdzVoo4kogiHWcRMB3YZ-iEdvfUQDdNdkB0Q9LcoZZPaPsQaaHn2RRCXRId70cylcSSE4Bc949aoqIoGdOTjl7kvk9Xy620lJWGcFfhcZqF97UketlgY9kGbJ5llNFdg51ZmIpJYebne5QTmEWw5Yh_YQuvGfJ8j87xbPEa9KrQ2OB6cg6ehKrZS9Q4AfnRy7\n",
      "\n",
      "\n",
      "EDM playlist created (1874 tracks)\n",
      "Pop playlist created (278 tracks)\n",
      "Electronic trap playlist created (668 tracks)\n",
      "Tech house playlist created (586 tracks)\n",
      "Indietronica playlist created (198 tracks)\n",
      "Electronic playlist created (1291 tracks)\n"
     ]
    }
   ],
   "source": [
    "token = util.prompt_for_user_token(username, 'playlist-modify-private', client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri)\n",
    "sp = spotipy.Spotify(auth=token)\n",
    "\n",
    "for name in playlist_tracks:\n",
    "    id = sp.user_playlist_create(username, name+' [Sortify]', public=False)['id']\n",
    "    for chunk in chunks(100, playlist_tracks[name]):\n",
    "        sp.user_playlist_add_tracks(username, id, chunk)\n",
    "    print('%s playlist created (%d tracks)' % (name, len(playlist_tracks[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clustered = {}\n",
    "for degree in degrees:\n",
    "    artist = degree[0]\n",
    "    cluster = artist_cluster[artist]\n",
    "    if cluster not in top_clustered:\n",
    "        top_clustered[cluster] = [degree]\n",
    "    else:\n",
    "        top_clustered[cluster].append(degree)\n",
    "        \n",
    "for cluster in top_clustered:\n",
    "    print(cluster_names[cluster])\n",
    "    for degree in top_clustered[cluster][:10]:\n",
    "        print(\"\\t%s: %d\" % (artist_names[degree[0]], degree[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(playlists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
